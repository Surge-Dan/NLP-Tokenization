# 双向匹配中文分词NLP-Tokenization
How to use Maximum Matching Algorithm for Bidirectional Chinese Word Segmentation.

# 目的
使用正向最大匹配算法、反向最大匹配算法和双向最大匹配算法进行中文分词。

输入为完整的句子，输出为分好词的句子。

# Usage
run main.py
